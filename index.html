<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>MyBlog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="MyBlog">
<meta property="og:url" content="https://bitlfy.github.io/index.html">
<meta property="og:site_name" content="MyBlog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Lin Fangyv">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="MyBlog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MyBlog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://bitlfy.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-GNN-Notes-LEC-10" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/24/GNN-Notes-LEC-10/" class="article-date">
  <time class="dt-published" datetime="2024-12-24T02:15:56.000Z" itemprop="datePublished">2024-12-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/24/GNN-Notes-LEC-10/">GNN-Notes-LEC-10</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Heterogeneous-Graph-and-Knowledge-Graph"><a href="#Heterogeneous-Graph-and-Knowledge-Graph" class="headerlink" title="Heterogeneous Graph and Knowledge Graph"></a>Heterogeneous Graph and Knowledge Graph</h2><h3 id="1-Heterogeneous-Graphs"><a href="#1-Heterogeneous-Graphs" class="headerlink" title="1. Heterogeneous Graphs"></a>1. Heterogeneous Graphs</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>G &#x3D; (V, E, R, T)</p>
<p>V : set of nodes, which have their types in set T</p>
<p>E : set of edges, which have their relations type in set R</p>
<p>R : set of relations</p>
<p>T : set of nodes’ type</p>
<h3 id="2-Relation-GCN"><a href="#2-Relation-GCN" class="headerlink" title="2. Relation GCN"></a>2. Relation GCN</h3><h4 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h4><p>$$<br>h_v^{(l+1)} &#x3D; \sigma(\sum_{r \in R}\sum_{u\in N_v^r}\frac1{c_{v,r}} W_r^{(l)}h_u^{(l)}+W_0^{(l)}h_v^{(l)})<br>$$</p>
<h4 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h4><h5 id="Block-Diagonal-Matrices"><a href="#Block-Diagonal-Matrices" class="headerlink" title="Block Diagonal Matrices"></a>Block Diagonal Matrices</h5><p>use B low-dim matrices to present $W_r^{(l)}$, which reduce parameters from $d^{(l)}\times d^{(l+1)}$ to $B\times \frac{d^{(l+1)}}B \times \frac{d^{(l)}}B$</p>
<h5 id="Basis-Learning"><a href="#Basis-Learning" class="headerlink" title="Basis Learning"></a>Basis Learning</h5><p>regard $W_r^{(l)} &#x3D; \sum_{b&#x3D;1}^B a_{rb}^{(l)}\cdot V_b^{(l)}$ , where $V_b^{(l)}$ is shared and each relation need only learn ${a_{rb}^{l}}$</p>
<h3 id="3-RGCN-For-Link-Prediction"><a href="#3-RGCN-For-Link-Prediction" class="headerlink" title="3. RGCN For Link Prediction"></a>3. RGCN For Link Prediction</h3><p><strong>Training</strong></p>
<ol>
<li>Split edges into training message edges, training supervision edges, validation edges and test edges</li>
<li>use RGCN to score the training supervision edges</li>
<li>create negative edges by perturbing the training supervision edges</li>
<li>use RGCN to score the negative edges</li>
<li>max the score of training supervision edges while min the score of negative edges</li>
</ol>
<p><strong>Evaluation</strong></p>
<ol>
<li>calculate the score of test edges</li>
<li>calculate the score of negative edges, which produced by perturbing the test edges and not belong to training message edges and training supervision edges</li>
<li>obtain the ranking RK of test edges</li>
<li>calculate metrics (Higher is Better)<ol>
<li>Hits K : 1[RK &lt;&#x3D; K]</li>
<li>Reciprocal Rank : 1 &#x2F; RK</li>
</ol>
</li>
</ol>
<h3 id="4-Knowledge-Graph-Definition"><a href="#4-Knowledge-Graph-Definition" class="headerlink" title="4. Knowledge Graph Definition"></a>4. Knowledge Graph Definition</h3><p>Nodes labeled with Types —— Entities with Types</p>
<p>Edges between Nodes —— Relations between Entities</p>
<h3 id="5-Knowledge-Completion-Task"><a href="#5-Knowledge-Completion-Task" class="headerlink" title="5. Knowledge Completion Task"></a>5. Knowledge Completion Task</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p>Given an enormous KG, and (head, relation), predict the missing tails</p>
<h4 id="Key-Idea-of-KG-Presentation"><a href="#Key-Idea-of-KG-Presentation" class="headerlink" title="Key Idea of KG Presentation"></a>Key Idea of KG Presentation</h4><ul>
<li>associate entities and relations with <strong>shallow embeddings</strong></li>
<li>embeddings of (h,r) should be close to embedding of t</li>
</ul>
<h4 id="Key-Question"><a href="#Key-Question" class="headerlink" title="Key Question"></a>Key Question</h4><ul>
<li>how to embed (h,r)</li>
<li>how to define closeness</li>
</ul>
<h4 id="Relation-Pattern"><a href="#Relation-Pattern" class="headerlink" title="Relation Pattern"></a>Relation Pattern</h4><p>Symmetric : if r(a,b) then r(b,a)</p>
<p>Antisymmetric : if r(a,b) then $\neg$r(b,a)</p>
<p>inverse : if r1(a,b) then r2(b,a)</p>
<p>composition &#x2F; transitive : if r1(a,b) and r2(b,c) then r3(a,c)</p>
<p>1-to-N : r(a,b1), r(a,b2), …, r(a,bn) is true</p>
<h4 id="TransE"><a href="#TransE" class="headerlink" title="TransE"></a>TransE</h4><p><strong>Intuition</strong></p>
<p>for a triple (h, r, t), we hope $h + r \approx t$ if given fact is true else $h + r \ne t$</p>
<p><strong>Scoring function</strong><br>$$<br>f_r(h,t) &#x3D; - |h+r-t|<br>$$<br><strong>Update Formulation</strong><br>$$<br>\sum_{[(h,l,t),(h’,l,t’)]\in batchT} \nabla [\gamma + d(h+l,t) - d(h’+l,t’)]\<br>$$<br><strong>Algorithm</strong></p>
<p>k is the dim of embeddings</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">TransE</span>(<span class="params">S:<span class="built_in">set</span>[&#123;h,l,t&#125;],E:<span class="built_in">set</span>[entities],L:<span class="built_in">set</span>[link],k:<span class="built_in">int</span>,</span>):</span><br><span class="line">    <span class="comment"># Initial</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> L:</span><br><span class="line">	    l = uniform(-<span class="number">6</span>/sqrt(k),-<span class="number">6</span>/sqrt(k))</span><br><span class="line">        l = l / ||l||</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> E:</span><br><span class="line">        e = uniform(-<span class="number">6</span>/sqrt(k),-<span class="number">6</span>/sqrt(k))</span><br><span class="line">    <span class="comment"># Loop</span></span><br><span class="line">    <span class="keyword">while</span> :</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> E :</span><br><span class="line">            e = e / || e ||</span><br><span class="line">        batchS = sample(S,batch_size)</span><br><span class="line">        batchT = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> (h,l,t) <span class="keyword">in</span> batchS:</span><br><span class="line">            h_n,l,t_n = neg_sample(S(h,l,t))</span><br><span class="line">            batchT.add([(h,l,t),(h_n,l,t_n)])</span><br><span class="line">        <span class="comment"># update</span></span><br><span class="line">        update <span class="keyword">with</span> formulation</span><br></pre></td></tr></table></figure>

<p><strong>expression power</strong></p>
<p>could express</p>
<ul>
<li>antisymmetric</li>
<li>inverse</li>
<li>composition</li>
</ul>
<p>but can’t express</p>
<ul>
<li>symmetric</li>
<li>1-to-N</li>
</ul>
<h4 id="TransR"><a href="#TransR" class="headerlink" title="TransR"></a>TransR</h4><p><strong>Intuition</strong></p>
<p>Entities in the Entity Space $\mathbb R^{d}$</p>
<p>Relations in the Relation Space $\mathbb R^k$</p>
<p>Projection matrix $M\in R^{d\times k}$ project entities to the Relation Space</p>
<p>For Entity h and t, we hope the projections of them in the Relation Space satisfy $h_\bot + l \approx t_\bot$</p>
<p><strong>Scoring Function</strong><br>$$<br>f_r(h,t) &#x3D; -|h_\bot +l-t_\bot|<br>$$<br><strong>Expression Power</strong></p>
<p>could express:</p>
<ul>
<li>antisymmetric</li>
<li>inverse</li>
<li>symmetric</li>
<li>1-to-N</li>
</ul>
<p>can’t express</p>
<ul>
<li>composition &#x2F; transition</li>
</ul>
<h4 id="Bilinear-Modeling-DistMult"><a href="#Bilinear-Modeling-DistMult" class="headerlink" title="Bilinear Modeling : DistMult"></a>Bilinear Modeling : DistMult</h4><p><strong>Intuition</strong></p>
<p>min the cosine similarity between h * r and t</p>
<p><strong>Scoring Function</strong><br>$$<br>f_r(h,t) &#x3D; \sum_i h_i \cdot l_i \cdot t_i<br>$$<br><strong>Expression Power</strong></p>
<p>could express</p>
<ul>
<li>symmetric</li>
<li>1-to-N</li>
</ul>
<p>can’t express</p>
<ul>
<li>antisymmetric</li>
<li>inverse</li>
<li>composition</li>
</ul>
<h4 id="ComplEx"><a href="#ComplEx" class="headerlink" title="ComplEx"></a>ComplEx</h4><p><strong>intuition</strong></p>
<p>embedding entities and relations in <strong>Complex vector space</strong></p>
<p><strong>Scoring Function</strong><br>$$<br>f_r(h,t) &#x3D; Re(\sum_i h_i \cdot r_i \cdot \bar t_i)\<br>t_i &#x3D; a + b i \Rightarrow \bar t_i &#x3D; a - b i<br>$$<br><strong>Expression Power</strong></p>
<p>could express</p>
<ul>
<li>symmetric</li>
<li>antisymmetric</li>
<li>inverse</li>
<li>1-to-N</li>
</ul>
<p>can’t express</p>
<ul>
<li>composition</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/24/GNN-Notes-LEC-10/" data-id="cm51u529w0009bww51ri0d589" data-title="GNN-Notes-LEC-10" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-09" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/24/GNN-Notes-LEC-09/" class="article-date">
  <time class="dt-published" datetime="2024-12-24T02:15:33.000Z" itemprop="datePublished">2024-12-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/24/GNN-Notes-LEC-09/">GNN-Notes-LEC-09</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Lec-09-Theory"><a href="#Lec-09-Theory" class="headerlink" title="Lec 09 Theory"></a>Lec 09 Theory</h2><h3 id="1-how-a-GNN-captures-local-neighborhood-structures"><a href="#1-how-a-GNN-captures-local-neighborhood-structures" class="headerlink" title="1. how a GNN  captures local neighborhood structures ?"></a>1. how a GNN  captures local neighborhood structures ?</h3><p>(1) GNN captures local neighborhood structures by <strong>computational graph</strong></p>
<p>(2) The key point of GNN expression is the <strong>aggregation function</strong>. In other word, most expressive GNN would use an <strong>injective function at each step</strong>.</p>
<h3 id="2-Most-Powerful-GNN-GIN"><a href="#2-Most-Powerful-GNN-GIN" class="headerlink" title="2. Most Powerful GNN : GIN"></a>2. Most Powerful GNN : GIN</h3><p>Observation : Neighbor aggregation can be abstracted as <strong>a function over a multi-set</strong> </p>
<h4 id="Problem-of-GCN-and-GraphSAGE"><a href="#Problem-of-GCN-and-GraphSAGE" class="headerlink" title="Problem of GCN and GraphSAGE"></a>Problem of GCN and GraphSAGE</h4><p>can’t distinguish different multi-sets</p>
<h4 id="Injective-Multi-Set-Function"><a href="#Injective-Multi-Set-Function" class="headerlink" title="Injective Multi-Set Function"></a>Injective Multi-Set Function</h4><p><strong>General injective multi-set function formulation</strong><br>$$<br>\Phi(\sum_{x\in S}f(x))<br>$$<br>where $f(x)$ produces one-hot encodings of colors.</p>
<p><strong>Universal Approximation Theorem</strong></p>
<p>1-hidden layer MLP with sufficiency-large hidden dimensionality and appropriate non-linearity can approximate any continuous function to an arbitrary accuracy</p>
<p><strong>Graph Isomorphism Network</strong><br>$$<br>MLP_{\Phi}(\sum_{x\in S}MLP_f(x))<br>$$<br>GIN is the most expressive GNN in the class of message-passing GNNs.</p>
<h3 id="3-Full-Model-of-GIN"><a href="#3-Full-Model-of-GIN" class="headerlink" title="3. Full Model of GIN"></a>3. Full Model of GIN</h3><p><strong>General injective function over tuple formulation</strong><br>$$<br>MLP_{\Phi}((1+\epsilon)\cdot MLP_f(c^{(k)}(v))+\sum_{u\in N(v)}MLP_f(c^{(k)}(u)))<br>$$<br>where $\epsilon$ is learn-able scalar</p>
<p><strong>GIN’s node updates</strong></p>
<ol>
<li><p>assign an initial vector $c^{(0)}(v)$ to each node $v$</p>
</li>
<li><p>Iteratively update node vectors by<br>$$<br>c^{(k+1)}(v) &#x3D; GINCONV({c^{(k)}(v),{c^{(k)}(u)}_{u\in N(v)}})<br>$$<br>where GINConv maps different inputs to different embeddings.</p>
</li>
<li><p>after K steps, $c^{(k)}(k)$ summarizes the structure of K-hop neighborhood</p>
</li>
</ol>
<p><strong>Improve GNN’s Power</strong> </p>
<p>There are basic graph structures that existing GNN  framework cannot distinguish, such as difference in cycles.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/24/GNN-Notes-LEC-09/" data-id="cm51u529x000abww5b1erfk1d" data-title="GNN-Notes-LEC-09" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-07-08" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/24/GNN-Notes-LEC-07-08/" class="article-date">
  <time class="dt-published" datetime="2024-12-24T02:15:14.000Z" itemprop="datePublished">2024-12-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/24/GNN-Notes-LEC-07-08/">GNN-Notes-LEC-07-08</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Lec-07-08-GNN"><a href="#Lec-07-08-GNN" class="headerlink" title="Lec 07 &amp; 08 GNN"></a>Lec 07 &amp; 08 GNN</h3><h3 id="1-A-General-GNN-Framework"><a href="#1-A-General-GNN-Framework" class="headerlink" title="1. A General GNN Framework"></a>1. A General GNN Framework</h3><ul>
<li>GNN layer<ul>
<li>Message</li>
<li>Aggregate</li>
</ul>
</li>
<li>Layer connectivity<ul>
<li>Stack layers sequentially</li>
<li>Ways of adding skip connections</li>
</ul>
</li>
<li>Graph Augmentation<ul>
<li>Raw input graph $\ne$ computational graph</li>
<li>Graph feature augmentation</li>
<li>Graph structure augmentation</li>
</ul>
</li>
<li>Learning Objective<ul>
<li>supervised &#x2F; unsupervised</li>
<li>node &#x2F; edge &#x2F;  graph level</li>
</ul>
</li>
</ul>
<h3 id="2-A-single-layer-of-a-GNN"><a href="#2-A-single-layer-of-a-GNN" class="headerlink" title="2. A single layer of a GNN"></a>2. A single layer of a GNN</h3><p>Idea : compress a set of vector into a single vector</p>
<h4 id="Message-computation"><a href="#Message-computation" class="headerlink" title="Message computation"></a>Message computation</h4><p>Idea : each node will create message and sent to other node later</p>
<p>Message function :<br>$$<br>m_u^{(l)} &#x3D; MSG^{(l)}(h_u^{(l-1)})),u\in N(v) \cup {v}<br>$$</p>
<h4 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h4><p>Idea : each node will aggregate the message from node $v$’s neighbors</p>
<p>Aggregation function :<br>$$<br>h_v^{(l)} &#x3D; AGG^{(l)}({m_u^{(l)},u\in N(v)},m_v^{(l)})<br>$$</p>
<h4 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h4><p>can be added to message computation or aggregation</p>
<h3 id="3-Various-GNN"><a href="#3-Various-GNN" class="headerlink" title="3. Various GNN"></a>3. Various GNN</h3><h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><p>$$<br>h_v^{(l)} &#x3D; \sigma(W^{(l)} \sum_{u\in N(v)}\frac{h_u^{(l-1)}}{|N(v)|})\<br>MSG^{(l)}(\cdot) &#x3D; W^{(l)}\frac{h_u^{(l-1)}}{|N(v)|}\<br>AGG^{(l)}(\cdot) &#x3D; \sum_{i \in N(v)} MSG^{(l)}(\cdot)<br>$$</p>
<h4 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h4><p>$$<br>h_v^{l} &#x3D;\sigma(W^{(l)}\cdot CONCAT(h_v^{(l-1)},AGG({h_u^{(l-1)},\forall u \in N(v)})))\<br>AGG(\cdot) \in {Mean(\cdot),Pool(\cdot),LSTM(\cdot)}\<br>h_v^{(l)}\leftarrow \frac{h_v^{(l)}}{|h_v^{(l)}|_2}<br>$$</p>
<h4 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h4><p>$$<br>h_v^l &#x3D; \sigma(\sum_{u\in N(v)}\alpha_{vu} W^{(l)}h_u^{(l-1)})\<br>\alpha_{vu} &#x3D; \frac{\exp(e_{vu})}{\sum_{k\in N(v)}\exp(e_{vk})}\<br>e_{vu} &#x3D; a(W^{(l)}h_u^{(l-1)},W^{(l)}h_v^{(l-1)})<br>$$</p>
<p>where a is attention mechanism function</p>
<h4 id="GAT-with-Multi-Attention"><a href="#GAT-with-Multi-Attention" class="headerlink" title="GAT with Multi-Attention"></a>GAT with Multi-Attention</h4><p>$$<br>h_v^{(l)}[1] &#x3D; \sigma(\sum_{u\in N(v)}\alpha_{vu}^1 W^{(l)}h_u^{(l-1)})\<br>h_v^{(l)}[2] &#x3D; \sigma(\sum_{u\in N(v)}\alpha_{vu}^2 W^{(l)}h_u^{(l-1)})\<br>h_v^{(l)}[3] &#x3D; \sigma(\sum_{u\in N(v)}\alpha_{vu}^3 W^{(l)}h_u^{(l-1)})\<br>h_v^{(l)} &#x3D; AGG(h_v^{(l)}[1],h_v^{(l)}[2],h_v^{(l)}[3])<br>$$</p>
<h3 id="4-GNN-Layer-in-Practice"><a href="#4-GNN-Layer-in-Practice" class="headerlink" title="4. GNN Layer in Practice"></a>4. GNN Layer in Practice</h3><ul>
<li><p>Linear</p>
</li>
<li><p>Batch Normalization</p>
<p>Stabilize neural network training<br>$$<br>\mu_j&#x3D;\frac1N\sum_{i&#x3D;1}^NX_{i,j}\<br>\sigma_j^2&#x3D;\frac1N\sum_{i&#x3D;1}^N(X_{i,j}-\mu_j)^2\<br>\widehat X_{i,j} &#x3D; \frac{X_{i,j}-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}\<br>Y_{i,j} &#x3D; \gamma_j \widehat X_{i,j} + \beta_j<br>$$</p>
</li>
<li><p>Dropout</p>
<p>prevent overfitting</p>
<p>During training : with some probability $p$, randomly set some neurons to zero</p>
<p>During testing : use all neurons to calculate</p>
</li>
<li><p>Activation</p>
<ul>
<li><p>ReLU<br>$$<br>ReLU(x_i) &#x3D; \max(x_i,0)<br>$$</p>
</li>
<li><p>Sigmoid<br>$$<br>\sigma(x_i) &#x3D; \frac 1 {1+e^{-x_i}}<br>$$</p>
</li>
<li><p>Parametric ReLU<br>$$<br>PReLU(x_i) &#x3D; \max(x_i,0) + a_i\min(x_i,0)<br>$$<br>where $a_i$ is a trainable parameter and it performs better than ReLU</p>
</li>
</ul>
</li>
<li><p>Attention &#x2F; Gating</p>
<p>Control the importance of a message</p>
</li>
<li><p>Aggregation</p>
</li>
</ul>
<h3 id="5-Stacking-Layers-of-a-GNN"><a href="#5-Stacking-Layers-of-a-GNN" class="headerlink" title="5. Stacking Layers of a GNN"></a>5. Stacking Layers of a GNN</h3><h4 id="Stack-layers-sequentially"><a href="#Stack-layers-sequentially" class="headerlink" title="Stack layers sequentially"></a>Stack layers sequentially</h4><p>$$<br>h_v^{(0)} &#x3D; x_v\<br>h_v^{(l)} &#x3D; GNN(h_v^{(l-1)})\<br>y &#x3D; h_v^{(L)}<br>$$</p>
<h4 id="Over-Smoothing-Problem"><a href="#Over-Smoothing-Problem" class="headerlink" title="Over-Smoothing Problem"></a><strong>Over-Smoothing Problem</strong></h4><p>all the node embeddings converge to the same value</p>
<p>due to the shared neighbors quickly grows when increase the number of hops</p>
<h4 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a><strong>Solutions</strong></h4><ul>
<li><p>be cautious when adding GNN layers : the L should be a bit more than the receptive field</p>
<p>But <strong>How to enhance the expressive power with less GNN layers</strong></p>
<ul>
<li><p>within GNN layer : make AGG &#x2F; Transformation become a DNN</p>
</li>
<li><p>add layers that no pass messages : Pre-processing layers, or post-processing layers</p>
</li>
</ul>
</li>
<li><p>Skip Connections (ResNet)</p>
<ul>
<li>Function</li>
</ul>
<p>$$<br>F(x)\rightarrow F(x) + x<br>$$</p>
<ul>
<li><p>Intuition</p>
<p>create a mixture of models</p>
</li>
</ul>
</li>
</ul>
<h3 id="6-Graph-Manipulation"><a href="#6-Graph-Manipulation" class="headerlink" title="6. Graph Manipulation"></a>6. Graph Manipulation</h3><h4 id="Why-Manipulate-Graphs"><a href="#Why-Manipulate-Graphs" class="headerlink" title="Why Manipulate Graphs"></a>Why Manipulate Graphs</h4><ul>
<li>Feature Level<ul>
<li>input graph lack features</li>
</ul>
</li>
<li>structure level<ul>
<li>graph is too sparse so that message passing is inefficient</li>
<li>graph is too dense so that message passing is too costly</li>
<li>graph is too large so that can not fit the computational graph into GPU</li>
</ul>
</li>
</ul>
<h4 id="Graph-Feature-Manipulation"><a href="#Graph-Feature-Manipulation" class="headerlink" title="Graph Feature Manipulation"></a>Graph Feature Manipulation</h4><p><strong>Reason</strong></p>
<ul>
<li>input graph does not have node features</li>
<li>certain structures are hard to learn by GNN, e.g. Cycle Graph</li>
</ul>
<p><strong>approaches</strong></p>
<ul>
<li>assign constant values to nodes</li>
<li>assign unique ids to nodes</li>
<li>add features by Clustering coefficient, PageRank, Centrality and so on</li>
</ul>
<h4 id="Augment-sparse-graphs"><a href="#Augment-sparse-graphs" class="headerlink" title="Augment sparse graphs"></a>Augment sparse graphs</h4><p><strong>Add virtual edges</strong></p>
<p>Intuition : use $A+A^2$ instead of using $A$</p>
<p>Use case : Bipartite graphs</p>
<p><strong>Add virtual nodes</strong></p>
<p>approach : add virtual node which connect all nodes in the graph</p>
<p>Benefits : greatly improves message passing</p>
<h4 id="Augment-Dense-Graphs"><a href="#Augment-Dense-Graphs" class="headerlink" title="Augment Dense Graphs"></a>Augment Dense Graphs</h4><p>approach : Neighborhood Sampling, that is, randomly sample a node’s neighborhood when compute the embeddings each time</p>
<p>Benefits : greatly reduce the computational cost</p>
<h3 id="7-Predict-With-GNN"><a href="#7-Predict-With-GNN" class="headerlink" title="7. Predict With GNN"></a>7. Predict With GNN</h3><h4 id="Node-Level-Prediction"><a href="#Node-Level-Prediction" class="headerlink" title="Node-Level Prediction"></a>Node-Level Prediction</h4><p>Input : d-dim node embeddings ${h_v^{(L)} \in \mathbb R^d,\forall v\in G}$</p>
<p>Output : $\widehat y_v &#x3D; Head_{node}(h_v^{(L)})&#x3D;W^{(H)}h_v^{(L)}$</p>
<h4 id="Edge-Level-Prediction"><a href="#Edge-Level-Prediction" class="headerlink" title="Edge-Level Prediction"></a>Edge-Level Prediction</h4><p>Output : $\widehat y_{uv} &#x3D; Head_{edge}(h_u^{(L)},h_v^{(L)})$</p>
<p><strong>Concatenation plus Linear</strong></p>
<p>$\widehat y_{uv} &#x3D; Linear(Concat(h_u^{(L)},h_v^{(L)}))$</p>
<p><strong>Dot Product</strong></p>
<p>1-way : $\widehat y_{uv} &#x3D; (h_u^{(L)})^T h_v^{(L)}$</p>
<p>k-way : $\widehat y_{uv}^{(i)} &#x3D; h_u^{(L)}W^{(i)}h_v^{(L)},i\in[1,k]$</p>
<h4 id="Graph-level-Prediction"><a href="#Graph-level-Prediction" class="headerlink" title="Graph-level Prediction"></a>Graph-level Prediction</h4><p>Output : $\widehat y_{G} &#x3D; Head_{graph}(h_v^{(L)}\in \mathbb R^d,\forall v\in G)$</p>
<p><strong>Global Pooling</strong></p>
<p>global mean &#x2F; max &#x2F; sum</p>
<p>Weakness :  lose information with large graph</p>
<p><strong>Hierarchical Global Pooling</strong></p>
<p>use $ReLU(Sum(\cdot))$ to hierarchically aggregate</p>
<p><strong>Differentiable Pooling</strong></p>
<p>GNN<br>$$<br>Z &#x3D; GNN(X,A)<br>$$<br>Differentiable Pooling GNN<br>$$<br>(A^{(l+1)},X^{(l+1)}) &#x3D; DIFFPOOL(A^{(l)},Z^{(l)})<br>$$</p>
<ul>
<li>compute the cluster that a node belong to</li>
<li>pooling in the cluster, and the edges between cluster would be generated</li>
<li>joint train GNN and Differentiable Pooling GNN</li>
</ul>
<h3 id="8-Graph-Split"><a href="#8-Graph-Split" class="headerlink" title="8. Graph Split"></a>8. Graph Split</h3><p><strong>Transductive setting</strong> </p>
<p>the input graph can be observed in all the dataset split, it means that we will only split the labels</p>
<p><strong>Inductive setting</strong></p>
<p>break the edges between splits to get multiple graphs</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/24/GNN-Notes-LEC-07-08/" data-id="cm51u529s0008bww5cjdk7f1o" data-title="GNN-Notes-LEC-07-08" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-06" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/24/GNN-Notes-LEC-06/" class="article-date">
  <time class="dt-published" datetime="2024-12-24T02:14:28.000Z" itemprop="datePublished">2024-12-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/24/GNN-Notes-LEC-06/">GNN-Notes-LEC-06</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Lec-06-Deep-Learning-for-Graphs"><a href="#Lec-06-Deep-Learning-for-Graphs" class="headerlink" title="Lec 06 Deep Learning for Graphs"></a>Lec 06 Deep Learning for Graphs</h2><h3 id="1-Problem-Definition"><a href="#1-Problem-Definition" class="headerlink" title="1. Problem Definition"></a>1. Problem Definition</h3><p>Given graph G : </p>
<p>$V$ : vertex set</p>
<p>$A$ : adjacency matrix</p>
<p>$X \in \mathbb R^{m\times|V|}$ : node features matrixs</p>
<p>$v$ : a node in vertex set</p>
<p>$N(v)$ : the neighbors of vertex $v$</p>
<h3 id="2-Neighborhood-Aggregation"><a href="#2-Neighborhood-Aggregation" class="headerlink" title="2. Neighborhood Aggregation"></a>2. Neighborhood Aggregation</h3><p><strong>Formulation of Scalar</strong><br>$$<br>h_v^0 &#x3D; x_v\<br>h_v^{(l+1)} &#x3D; \sigma(W_l\sum_{u\in N(v)}\frac{h_u^{(l)}}{|N(v)|}+B_lh_v^{(l)}),\forall l\in{0,1,…}\<br>z_v &#x3D; h_v^{(L)}<br>$$<br><strong>Formulation of Matrix</strong><br>$$<br>H^{(l)} &#x3D; [h_1^{(l)},h_2^{(l)},…,h_{|V|}^{(l)}]\<br>\sum_{u\in N(v)} h_u^{(l)} &#x3D; A \cdot H^{(l)}\<br>D\in R^{|V|\times |V|},\ where\ D_{v,v}&#x3D;|N(v)|\<br>H^{(l+1)} &#x3D; \sigma(W_lD^{-1}AH^{(l)}+H^{(l)}B_l^T)<br>$$<br><strong>Advantage</strong> : on the fly</p>
<h3 id="3-How-to-train-GNN"><a href="#3-How-to-train-GNN" class="headerlink" title="3. How to train GNN?"></a>3. How to train GNN?</h3><p><strong>Supervised setting</strong><br>$$<br>\min_{\Theta} \mathcal L(y,f(z_v))<br>$$<br><strong>Unsupervised setting</strong></p>
<p>Idea : use the graph structure as the supervision<br>$$<br>\mathcal L &#x3D; \sum_{z_u,z_v}CE(y_{u,v},DEC(z_u,z_v))<br>$$</p>
<h3 id="4-GraphSAGE-GCN"><a href="#4-GraphSAGE-GCN" class="headerlink" title="4. GraphSAGE &#x2F; GCN"></a>4. GraphSAGE &#x2F; GCN</h3><p><strong>Ideas of GraphSAGE</strong></p>
<ol>
<li>flexible aggregation function and  concatenation</li>
</ol>
<p>$$<br>h_v^{(l+1)} &#x3D; \sigma([W_l\cdot AGG({h_u^{(l)},\forall u \in N(v)}),B_lh_v^{(l)}])<br>$$</p>
<ol start="2">
<li>$l_2$ normalization<br>$$<br>h_v^k \leftarrow \frac{h_v^k}{|h_v^k|_2}<br>$$</li>
</ol>
<p><strong>All kinds of AGG</strong></p>
<ol>
<li><p>Mean<br>$$<br>AGG({h_u^{(l)},\forall u \in N(v)}) &#x3D; \sum_{u\in N(v)}\frac{h_u^{(l)}}{|N(v)|}<br>$$</p>
</li>
<li><p>Pool<br>$$<br>AGG({h_u^{(l)},\forall u \in N(v)}) &#x3D; \gamma({MLP(h_u^{(l)}),\forall u \in N(v)})<br>$$<br>$\gamma$ is element-wise mean or max</p>
</li>
<li><p>LSTM<br>$$<br>AGG({h_u^{(l)},\forall u \in N(v)}) &#x3D; LSTM([h_u^{(l)},\forall u \in (N(v))])<br>$$</p>
</li>
<li></li>
</ol>
<p>In fact, the AGG is the convolution of graph</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/24/GNN-Notes-LEC-06/" data-id="cm51u529q0007bww59qd863rn" data-title="GNN-Notes-LEC-06" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-05" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/15/GNN-Notes-LEC-05/" class="article-date">
  <time class="dt-published" datetime="2024-12-15T09:56:42.000Z" itemprop="datePublished">2024-12-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/15/GNN-Notes-LEC-05/">GNN-Notes-LEC-05</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Lec-05-Collective-Classification"><a href="#Lec-05-Collective-Classification" class="headerlink" title="Lec 05 Collective Classification"></a>Lec 05 Collective Classification</h2><h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>Question : Given a graph with labels on some nodes, how do we assign labels to all other nodes in the network ?</p>
<p>Intuition : Similar nodes are connected</p>
<p>Technologies : Relational classification, Iterative Classification, Belief Propagation</p>
<p> Explain of “Similar nodes are connected” from the view of social science:</p>
<ul>
<li><p>Homophily : the tendency of individuals to associate and bond with similar others</p>
</li>
<li><p>Influence : social connection can influence the individual characteristics of a person</p>
</li>
</ul>
<h3 id="2-Motivations-of-classification"><a href="#2-Motivations-of-classification" class="headerlink" title="2. Motivations of classification"></a>2. Motivations of classification</h3><ul>
<li><p>similar nodes are typically close together or directly connected in the network</p>
</li>
<li><p>classification label of node $v$ may depend on 1) features of $v$; 2) labels of the nodes in $v$’s neighborhood; 3) features of the nodes in $v$’s neighborhood</p>
</li>
</ul>
<h3 id="3-Problem-definition"><a href="#3-Problem-definition" class="headerlink" title="3. Problem definition"></a>3. Problem definition</h3><p>Given :</p>
<p>adjacency matrix $A$ over $n$ nodes</p>
<p>some nodes with labels $Y_i$ and some unlabeled nodes</p>
<p>the features of nodes $F$</p>
<p>Goal :</p>
<p>Predict the labels of unlabeled nodes, or find $P(Y_v)$ given features and network</p>
<h3 id="4-Pipeline-of-Collective-Classification"><a href="#4-Pipeline-of-Collective-Classification" class="headerlink" title="4. Pipeline of Collective Classification"></a>4. Pipeline of Collective Classification</h3><p>Hypothesis : Markov Assumption</p>
<p>Pipeline : </p>
<ul>
<li>Local classifier : used for initial label assignment<ul>
<li>Predict the nodes label used only their features, but not network information</li>
</ul>
</li>
<li>Relational Classifier : capture correlation<ul>
<li>label one node based on the labels or features of its neighbors</li>
</ul>
</li>
<li>Collective Inference<ul>
<li>apply the relational classifier to each node iteratively until the inconsistency between neighboring labels is minimized</li>
<li>be affected by network structure</li>
</ul>
</li>
</ul>
<h3 id="5-Relational-Classification"><a href="#5-Relational-Classification" class="headerlink" title="5. Relational Classification"></a>5. Relational Classification</h3><p>Base Idea : class probability of $Y_v$ of node $v$ is a weighted average of class probabilities of its neighbors</p>
<p>Steps : </p>
<ul>
<li><p>Initial : initial labeled node $v$ with its ground-truth label $Y_v^*$ while initial unlabeled nodes $Y_v &#x3D; 0.5$</p>
</li>
<li><p>Update all nodes’ labels with formula follow until convergence:<br>$$<br>P(Y_v &#x3D; c) &#x3D; \frac 1 {\sum_{(v,u)\in E}A_{u,v}}\sum_{(v,u)\in E}A_{v,u} P(Y_u&#x3D;c)<br>$$<br>where $A$ is adjacency matrix with weight or strength information</p>
</li>
</ul>
<p>Challenges :</p>
<p>Convergence is not guaranteed</p>
<p>model can’t use node feature information</p>
<h3 id="6-Iterative-Classification"><a href="#6-Iterative-Classification" class="headerlink" title="6. Iterative Classification"></a>6. Iterative Classification</h3><p><strong>Main Idea</strong> : classify node $v$ with its attributions $f_v$ and the summary of labels $z_v$ of neighbors set $N_v$ </p>
<p><strong>Approach</strong> : </p>
<ul>
<li><p>$\phi_1(f_v)$ : predict the label of node $v$ base of $f_v$</p>
</li>
<li><p>$\phi_2(f_v,z_v)$ : predict the label base of $f_v$ and $z_v$</p>
</li>
<li><p>the approaches of $z_v$</p>
<ul>
<li>histogram of the number of each label in $N_v$</li>
<li>most common label in $N_v$</li>
<li>number of different labels in $N_v$</li>
</ul>
</li>
</ul>
<p><strong>Steps</strong> :</p>
<ul>
<li>classify based on node attributions alone<ul>
<li>train classifier $\phi_1,\phi_2$ with training set</li>
</ul>
</li>
<li>Iterate till convergence<ul>
<li>Initial : initial the labels with $\phi_1$</li>
<li>Iterate : update $z_v$ based on $Y_u,\forall u \in N_v$ and then update $Y_v$ based on $f_v$ and $z_v$</li>
</ul>
</li>
</ul>
<p><strong>Challenges</strong> :</p>
<p>Convergence is not guaranteed</p>
<h3 id="7-Loopy-belief-propagation"><a href="#7-Loopy-belief-propagation" class="headerlink" title="7. Loopy belief propagation"></a>7. Loopy belief propagation</h3><p><strong>Notation</strong></p>
<ul>
<li>Label-label potential matrix $\psi$ : $\psi(Y_i,Y_j)$ is proportional to the probability of a node $j$ being in class $Y_j$ given that it has neighbor $i$ in $Y_i$</li>
<li>Prior belief $\phi$ : $\phi(Y_i)$ is proportional to the probability of node $i$ being in class $Y_i$</li>
<li>$m_{i\rightarrow j}(Y_j)$ : $i$’s message of $j$ being in $Y_j$</li>
<li>$\mathcal L$ : set of all classes &#x2F; labels</li>
</ul>
<p><strong>Steps</strong></p>
<ul>
<li><p>Initial all messages to 1</p>
</li>
<li><p>repeat for each node by formula<br>$$<br>m_{i\rightarrow j}(Y_j) &#x3D; \sum_{Y_i\in \mathcal L} \psi(Y_i,Y_j) \phi(Y_i) \Pi_{k\in N_i&#x2F;j} m_{k\rightarrow i}(Y_i),\forall Y_j \in \mathcal L<br>$$</p>
</li>
</ul>
<p><strong>Result</strong><br>$$<br>b_i(Y_i) &#x3D; \phi_i(Y_i)\Pi_{j \in N_i}m_{j\rightarrow i}(Y_i),\forall Y_i \in \mathcal L<br>$$<br><strong>Advantages</strong></p>
<ul>
<li>easy to program and parallelize</li>
<li>can apply to any graph model with any form of potentials</li>
</ul>
<p><strong>Challenges</strong></p>
<ul>
<li>convergence is not guaranteed, especially if many closed loops</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/15/GNN-Notes-LEC-05/" data-id="cm51u529p0006bww5ggamdpm0" data-title="GNN-Notes-LEC-05" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Certified-Robustness-inspired-Attack-Framework" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/12/Certified-Robustness-inspired-Attack-Framework/" class="article-date">
  <time class="dt-published" datetime="2024-12-12T12:19:03.000Z" itemprop="datePublished">2024-12-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/12/Certified-Robustness-inspired-Attack-Framework/">Certified Robustness inspired Attack Framework</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Turning-Strengths-into-Weaknesses-A-Certified-Robustness-Inspired-Attack-Framework-against-Graph-Neural-Networks"><a href="#Turning-Strengths-into-Weaknesses-A-Certified-Robustness-Inspired-Attack-Framework-against-Graph-Neural-Networks" class="headerlink" title="Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks"></a>Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul>
<li>GNN have achieved state-of-the-art performance in many graph tasks</li>
<li>However, GNNs are vulnerable to test-time evasion and training-time poisoning attacks</li>
<li>Certified robustness is used to defend adversarial attacks, but authors use its properties to attack</li>
<li>For a node with larger certified robustness, it is more robust to graph perturbations. So nodes with smaller certified robustness are more easy to attack.</li>
<li>Contribution : Design a certified robustness inspired attack loss, and produces its counterpart.</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>Background</strong></p>
<ol>
<li><p>The development of GNNs : many methods on GNNs have achieved state-of-the-art</p>
</li>
<li><p>Attacks : GNNs are vulnerable to test-time graph evasion attacks and training-time graph poisoning attacks</p>
<ul>
<li>Graph Evasion : Given a learned GNNs model and a clean graph, attacker perturbs the graph structure to make model make mistakes</li>
<li>Graph Poison : Given a GNN algorithm and a graph, attacker perturbs the graph structure in the training phase so that the model make mistakes in test-time</li>
</ul>
</li>
</ol>
<p><strong>Contribution &#x2F; FrameWork</strong></p>
<ol>
<li><p>generalize randomized smoothing and derive the node’s certified perturbation size, which inspire us to focus more on disrupting nodes with smaller certified perturbation size</p>
</li>
<li><p>Design a certified robustness inspired attack loss : modify the weights for different nodes, which means nodes with smaller certified robustness would be assigned larger weight while the larger nodes would be assigned smaller weight. So that more perturbation budget would be allocated to these large weighted nodes.</p>
</li>
<li><p>Design certified robustness attack inspired attack framework that only modify the existing attack loss with certified perturbation size defined node weights. So that any attack existing evasion and poison method could regarded as the base attack of the framework</p>
</li>
</ol>
<h3 id="Background-and-Preliminaries"><a href="#Background-and-Preliminaries" class="headerlink" title="Background and Preliminaries"></a>Background and Preliminaries</h3><p><strong>GNN : node classification</strong></p>
<p>$\mathcal V$ : nodes set</p>
<p>$u \in \mathcal V$ : node in nodes set</p>
<p>$\varepsilon$ : edges set</p>
<p>$(u,v) \in \varepsilon$ : edge in edges set</p>
<p>$G &#x3D; (\mathcal V,\varepsilon)$ : graph</p>
<p>$A \in {0,1}^{|\mathcal V|\times|\mathcal V|}$ : adjacency matrix</p>
<p>$\mathcal Y$ : nodes’ labels set</p>
<p>$y_u \in \mathcal Y$ : the label of node u</p>
<p>$\mathcal V_{Tr},\mathcal V_{Te}$ : the train &#x2F; test nodes set</p>
<p>$\mathcal A$ : the algorithm of GNN</p>
<p>$f_\theta &#x3D; \mathcal A(A,\mathcal V_{Tr}) : G(A)\rightarrow \mathcal Y^{|\mathcal V|}$ : the node classifier with its parameters $\theta$, training with graph and training nodes</p>
<p>Loss function : $\min_\theta \mathcal L(f_\theta,A,\mathcal V_{Tr}) &#x3D; \sum_{u\in \mathcal V_{Tr}} l(f_\theta,A,y_u)$</p>
<p><strong>Adversarial Attacks to GNNs</strong></p>
<p>$\delta \in {0,1}^{|\mathcal V|\times|\mathcal V|}$ : perturbations, the element $\delta_{s,t}$ means change the status of edge $(s,t)$</p>
<p>$A \oplus \delta$ : adjacency matrix of graph make XOR with perturbations</p>
<p>$\Delta \ge |\delta|$ : perturbations budget</p>
<ol>
<li><p>evasion attack</p>
<p>The attacker aims to maximize the loss follows :<br>$$<br>\max_{\delta} \sum_{v \in \mathcal V_{Te}} \mathbf 1[f_\theta(A\oplus\delta;v)\ne y_v],s.t. |\delta|\le \Delta<br>$$<br>Due to the problem above is challenging to solve, optimize the loss below :<br>$$<br>\max_\delta \sum_{v\in \mathcal V_{Te}} l(f_\theta(A\oplus\delta;v)\ne y_v),s.t. |\delta|\le \Delta<br>$$</p>
</li>
<li><p>poison attack</p>
<p>The attacker aims to solve the bilevel optimization problem :<br>$$<br>\max_{\delta} \sum_{v \in \mathcal V_{Te}} \mathbf 1[f_\theta(A\oplus\delta;v)\ne y_v]\<br>s.t. \theta &#x3D; \arg \min_\theta \sum_{u \in \mathcal V_{Tr}}\mathbf 1[f_\theta(A\oplus\delta;u)\ne y_u],|\delta|\le \Delta<br>$$<br>In practice, the label of test set is unavailable during training. As a result, the optimization problem above can’t solve and it is hard to solve in fact. Consequently, we optimize the problem below instead.<br>$$<br>\max_{\delta} \sum_{v \in \mathcal V_{Tr}}  l[f_\theta(A\oplus\delta;v)\ne y_v]\<br>s.t. \theta &#x3D; \arg \min_\theta \sum_{u \in \mathcal V_{Tr}} l[f_\theta(A\oplus\delta;u)\ne y_u],|\delta|\le \Delta<br>$$</p>
</li>
</ol>
<p><strong>Certified Robustness to Graph Evasion Attacks</strong></p>
<p>The random smoothing that defends against graph evasion attacks to GNNs consists of the three parts : </p>
<ol>
<li><p>construct smoothed node classifier</p>
<p>Given base classifier $f$, graph $G$, and test node $u$ with its label $y_u$. Then randomized smoothing node classifier g as follow :<br>$$<br>g(A;u) &#x3D; \arg \max_{c\in \mathcal Y}Pr(f(A\oplus \varepsilon;u)&#x3D;c)\<br>Pr(\varepsilon_{s,t}&#x3D;0) &#x3D; \beta,Pr(\varepsilon_{s,t}&#x3D;1) &#x3D; 1-\beta,<br>$$</p>
</li>
<li><p>Deriving the certified robustness of graph evasion attacks of GNNs</p>
<p>$g(A;u)&#x3D;y_u$ means $g$ predicts correctly the label of u. Then $g$ provably predicts the correct label for $u$ if $\delta$ is bounded.<br>$$<br>g(A\oplus \delta;u)&#x3D;y_u,\forall \delta|\delta|<em>0\le K(\underline{p</em>{y_u}})<br>$$<br>$\underline{p_{y_u}}$is the lower bound of the probability that f predict the correct label of u on the noisy $\varepsilon$ </p>
<p>$K(\underline{p_{y_u}})$ is the certified robustness of the node u.</p>
</li>
<li><p>Computing the certified perturbation size in practice</p>
<p>1)sample perturbations $\varepsilon^1,\varepsilon^2,…,\varepsilon^{n}$ from the distribution $Pr(\varepsilon_{s,t}&#x3D;0) &#x3D; \beta,Pr(\varepsilon_{s,t}&#x3D;1) &#x3D; 1-\beta$</p>
<p>2)add perturbations to $A$ and get $A\oplus \varepsilon^1,A\oplus \varepsilon^2,…,A\oplus \varepsilon^n$</p>
<p>3)use f to predict $u$’s label and counts the numbers of each label by $N_c &#x3D; \sum_{j&#x3D;1}^N \mathbb I(f(A\oplus \varepsilon,u)&#x3D;c),c\in \mathcal Y$</p>
<p>4)compute $\underline{p_{y_u}} &#x3D; B(\alpha,N_{y_u},N-N_{y_u}-1)$, where $1-\alpha$ is confidence level and $B(\alpha,a,b)$ is the $\alpha$-th quantile of Beta distribution with shape parameters a and b.</p>
</li>
</ol>
<h3 id="Certified-Robustness-to-Graph-Poisoning-Attacks-via-randomized-smoothing"><a href="#Certified-Robustness-to-Graph-Poisoning-Attacks-via-randomized-smoothing" class="headerlink" title="Certified Robustness to Graph Poisoning Attacks via randomized smoothing"></a>Certified Robustness to Graph Poisoning Attacks via randomized smoothing</h3><p><strong>Main Idea</strong></p>
<p>extend randomized smoothing from the classifier to a general function</p>
<p><strong>Building base function</strong></p>
<p>Define $\widetilde f(A,\mathcal V_{Tr};v)$ as the composition of 1) training the model with graph $G(A)$, algorithm $\mathcal A$ and training nodes set $\mathcal V_{Tr}$ ; 2) test the model with node $v$</p>
<p>View $\widetilde f$ as the base function.</p>
<p><strong>Construct a smoothed function</strong></p>
<p>Define the randomized smoothed function as follow :<br>$$<br>\widetilde g(A,\mathcal V_{Tr};v) &#x3D; \arg \max_{c\in \mathcal Y} Pr(\widetilde f(A\oplus \epsilon,\mathcal V_{Tr};v)&#x3D;c)<br>$$<br>where</p>
<p>$\epsilon \in {0,1}^{|\mathcal V|\times |\mathcal V|}$ : noise matrix whose element $\epsilon_{s,t}$ drawn from discrete distribution</p>
<p><strong>Deriving the certified robustness of graph poisoning attacks of GNNs</strong><br>$$<br>\widetilde g(A\oplus \delta,\mathcal V_{Tr};v)&#x3D;y_v,\forall |\delta|<em>0\le K(p</em>{y_v})<br>$$<br><strong>computing the certified perturbation size in practice</strong></p>
<p>Given : algorithm $\mathcal A$, graph $G(A)$, training nodes $\mathcal V_{Tr}$, and discrete distribution $Pr(\varepsilon_{s,t}&#x3D;0) &#x3D; \beta,Pr(\varepsilon_{s,t}&#x3D;1) &#x3D; 1-\beta$, and a test node $v$</p>
<p>Steps : </p>
<ol>
<li>sample $N$ random noise matrices $\epsilon^1,…,\epsilon^N$ from the discrete distribution given</li>
<li>add noise matrix to adjacency matrix and get $A\oplus\epsilon^1,…,A\oplus \epsilon^N$</li>
<li>train the classifiers  $\widetilde f^1 &#x3D; \mathcal A(A\oplus\epsilon^1,\mathcal V_{Tr},…,\widetilde f^N &#x3D; \mathcal A(A\oplus\epsilon^N,\mathcal V_{Tr})$ </li>
<li>use the classifiers predict v’s labels and count the number of all kinds of labels, $N_c &#x3D; \sum_{j&#x3D;1}^N\mathbb I(\widetilde f^j(A\oplus\epsilon^j,\mathcal V_{Tr};v)&#x3D;c),c\in \mathcal Y$ </li>
<li>estimate $\underline{p_{y_v}}$ by $\underline{p_{y_v}} &#x3D; B(\alpha,N_{y_v},N-N_{y_v}-1)$ and calculate the perturbation size</li>
</ol>
<h3 id="Certified-Robustness-Inspired-Attack-Framework-against-GNNs"><a href="#Certified-Robustness-Inspired-Attack-Framework-against-GNNs" class="headerlink" title="Certified Robustness Inspired Attack Framework against GNNs"></a>Certified Robustness Inspired Attack Framework against GNNs</h3><h4 id="Observation"><a href="#Observation" class="headerlink" title="Observation"></a>Observation</h4><p><strong>Observation</strong></p>
<p>A node with a larger(smaller) certified perturbation size should be disrupted with a smaller(larger) number of perturbation edges.</p>
<p><strong>Idea</strong></p>
<p>With a perturbation budget, an attacker should avoid disrupting nodes with larger certified perturbation sizes, but focus on the nodes with smaller perturbation size.</p>
<p><strong>Questions and Measures</strong></p>
<ol>
<li>How to get the perturbation sizes ? : derived node’s certified perturbation size</li>
<li>How to allocate the budget for the nodes ? :  a certified perturbation inspired loss</li>
<li>How to generate the perturbation ? : certified robustness inspired attack framework</li>
</ol>
<h4 id="Certified-Robustness-Inspired-Loss-Design"><a href="#Certified-Robustness-Inspired-Loss-Design" class="headerlink" title="Certified Robustness Inspired Loss Design"></a>Certified Robustness Inspired Loss Design</h4><p><strong>Naive solution and its weakness</strong></p>
<p>Solution : sorts all nodes’ certified robustness sizes in an ascending order and perturbs them one-by-one until reaching the budget.</p>
<p>Weakness : computationally intensive and suboptimal </p>
<p><strong>Idea</strong></p>
<p>The loss functions in the poison and evasion are defined for each nodes. So that, assign each node with a weight which associated with its certified perturbation. The loss function as follow :<br>$$<br>L_{CR}(f_\theta,A,\mathcal V_T) &#x3D; \sum_{u \in V_T} w(u)\cdot \mathcal l(f_\theta(A;u),y_u)\<br>w(u) &#x3D; \frac{1}{1+\exp(a\cdot K(\underline{p_{y_u}}))}<br>$$<br>where $a$ is super parameter</p>
<h4 id="Certified-Robustness-Inspired-Attack-Design"><a href="#Certified-Robustness-Inspired-Attack-Design" class="headerlink" title="Certified Robustness Inspired Attack Design"></a>Certified Robustness Inspired Attack Design</h4><p><strong>certified robustness inspired evasion attacks to generate graph perturbations</strong></p>
<p>For any base attack, only modify the loss by multiplying it with certification perturbation sizes defined node weights.</p>
<p>For instance, use PGD attack. The perturbation should iteratively generates by<br>$$<br>\delta &#x3D; Proj_{\mathbb B}(\delta+\eta \cdot \nabla_\delta \mathcal L_{CR}(f_\theta,A\oplus \delta, \mathcal V_{Te})\<br>\mathbb B&#x3D;{\delta:1^T\cdot \delta\le \Delta,\delta\in[0,1]^{|\mathcal V|\times\mathcal V|}}\<br>Proj_{\mathbb B}(a) &#x3D; \left{ \begin{array}{rcl}<br>\Pi_{[0,1]}(a-\mu1),1^T\Pi_{[0,1]}(a-\mu1)&#x3D;\Delta\<br>\Pi_{[0,1]}(a),1^T\Pi_{[0,1]}(a-\mu1)\le\Delta<br>\end{array}\right.<br>$$<br><strong>certified robustness inspired graph poisoning attacks to generate graph perturbations</strong><br>$$<br>\max_\delta \mathcal L_{CR}(f_\theta*,A\oplus\delta,\mathcal V_{Tr})\<br>s.t. \theta^* &#x3D; \arg\min_{\theta} \mathcal L_{CR}(f_\theta,A\oplus\delta,\mathcal V_{Tr}),|\delta|\le \Delta<br>$$<br>问题：</p>
<ol>
<li>将随机平滑从分类器扩展到一般函数，似乎不是很明显？</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/12/Certified-Robustness-inspired-Attack-Framework/" data-id="cm51u529f0001bww5g2t82rlo" data-title="Certified Robustness inspired Attack Framework" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Certified-Robustness-via-Randomized-Smoothing" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/11/Certified-Robustness-via-Randomized-Smoothing/" class="article-date">
  <time class="dt-published" datetime="2024-12-11T03:43:55.000Z" itemprop="datePublished">2024-12-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/11/Certified-Robustness-via-Randomized-Smoothing/">Certified Robustness via Randomized Smoothing</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="可证对抗鲁棒性"><a href="#可证对抗鲁棒性" class="headerlink" title="可证对抗鲁棒性"></a>可证对抗鲁棒性</h2><p>本文与其他<strong>以Idea+实验+可解释性为主的</strong>论文不同，其以<strong>公式推导和数学证明</strong>为核心。因此，此笔记不使用往常顺延论文逻辑脉络的写法，意在整理整个推到过程。同时，略去一些引论的证明，只给出最终结论，目的在于突出重点，使论文证明清晰。</p>
<h3 id="第一部分-：-严密可证对抗鲁棒性"><a href="#第一部分-：-严密可证对抗鲁棒性" class="headerlink" title="第一部分 ： 严密可证对抗鲁棒性"></a>第一部分 ： 严密可证对抗鲁棒性</h3><p>该部分分为两个定理.</p>
<p>定理一证明的是，在扰动小于某个特定半径R的情况下，分类器一定具有鲁棒性。</p>
<p>定理二证明的是，在扰动半径大于该特定半径R的情况下，一定能找到一个特定的f，使平滑后的g不具有鲁棒性。即从函数的角度证明了其严密性。</p>
<p>最终得到的扰动边界为<br>$$<br>R &#x3D; \frac \sigma 2 [\Phi^{-1}(\underline {p_A})-\Phi^{-1}(\overline{p_B})]\<br>\forall |\delta|&lt;R,g(x+\delta)&#x3D;c_A\<br>\forall |\delta|&gt;R,\exist f,g : g(x+\delta)\ne c_A<br>$$<br><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%AE%9A%E7%90%861-1.jpg"></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%AE%9A%E7%90%861-2.jpg"></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%AE%9A%E7%90%861-3.jpg"></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%AE%9A%E7%90%862.jpg"></p>
<h3 id="第二部分-：-在线性二分类分类器上证明对抗鲁棒性及扩展"><a href="#第二部分-：-在线性二分类分类器上证明对抗鲁棒性及扩展" class="headerlink" title="第二部分 ： 在线性二分类分类器上证明对抗鲁棒性及扩展"></a>第二部分 ： 在线性二分类分类器上证明对抗鲁棒性及扩展</h3><p>第二部分通过四个命题，陈述了可证对抗鲁棒性在线性二分类分类器上的情况。并对无穷半径的情况进行说明。</p>
<p>命题一证明 ：对于线性二分类分类器，其随机平滑的结果与原函数一致。</p>
<p>命题二证明 ：对于线性二分类分类器，其扰动半径等于样本点到决策边界的距离，符合实际，佐证了定理一</p>
<p>命题三证明 ：线性二分类分类器对于大于扰动半径的扰动，其不再具有鲁棒性。与定理二不同，命题三从“距离”的角度，证明了可证扰动半径的紧密性。</p>
<p>命题四证明 ：存在函数和输入x，使可证扰动半径达到无穷。</p>
<p><strong>命题一</strong></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%981.jpg"></p>
<p><strong>命题二</strong></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%982.jpg"></p>
<p><strong>命题三</strong></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%983.jpg"></p>
<p><strong>命题四</strong></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%984.jpg"></p>
<h3 id="第三部分-：-可证扰动半径在高分辨率图像的可拓展性"><a href="#第三部分-：-可证扰动半径在高分辨率图像的可拓展性" class="headerlink" title="第三部分 ： 可证扰动半径在高分辨率图像的可拓展性"></a>第三部分 ： 可证扰动半径在高分辨率图像的可拓展性</h3><p>由于可证扰动半径与维度无关，作者额外证明了，在同一张图像高分辨率和低分辨率的情况下，高分辨率具有更高的扰动半径。</p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%985.jpg"></p>
<h3 id="第四部分-：-针对可证扰动半径提出实用算法"><a href="#第四部分-：-针对可证扰动半径提出实用算法" class="headerlink" title="第四部分 ： 针对可证扰动半径提出实用算法"></a>第四部分 ： 针对可证扰动半径提出实用算法</h3><p>该部分首先提出了实用算法，对于实用算法的“实用性”，命题6和命题7进行了证明。</p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%AE%9E%E7%94%A8%E7%AE%97%E6%B3%95.jpg"></p>
<p><strong>命题6</strong></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%986.jpg"></p>
<p><strong>命题7</strong></p>
<p><img src="/../images/Certified-Robustness-via-Randomized-Smoothing/%E5%91%BD%E9%A2%987.jpg"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/11/Certified-Robustness-via-Randomized-Smoothing/" data-id="cm51u52960000bww5huhj0a1d" data-title="Certified Robustness via Randomized Smoothing" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-04" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/06/GNN-Notes-LEC-04/" class="article-date">
  <time class="dt-published" datetime="2024-12-06T11:49:12.000Z" itemprop="datePublished">2024-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/06/GNN-Notes-LEC-04/">GNN Notes : LEC 04</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Lec-04-PageRank-and-Link-Analysis"><a href="#Lec-04-PageRank-and-Link-Analysis" class="headerlink" title="Lec 04 PageRank and Link Analysis"></a>Lec 04 PageRank and Link Analysis</h2><h3 id="1-PageRank"><a href="#1-PageRank" class="headerlink" title="1. PageRank"></a>1. PageRank</h3><h4 id="main-idea"><a href="#main-idea" class="headerlink" title="main idea"></a>main idea</h4><p>Regard links as votes : Page is more important if it has more links</p>
<p>Different Importance of links : links from important links are more important</p>
<p>Combination : A point is important if it is pointed by other important pages</p>
<h4 id="Formula"><a href="#Formula" class="headerlink" title="Formula"></a>Formula</h4><p><strong>Common Formulation</strong></p>
<p>$r_i$ : the $i$-th node’s rank or score</p>
<p>$d_i$ : the out-degree of $i$-th node<br>$$<br>r_j &#x3D; \sum_{i-&gt;j}\frac{r_i}{d_i}<br>$$<br><strong>Matrix Formulation</strong></p>
<p>$A_{ij}$ : equal to 1 if edge from node $i$ to $j$ exists, else 0</p>
<p>$sum_by_row(A)$ : a vector means the out-degrees of nodes in graph</p>
<p>Stochastic adjacency matrix M : if $j-&gt;i$, $M_{ij} &#x3D; 1 &#x2F; d_j$</p>
<p>$M &#x3D; (A &#x2F; sum_by_row(A))^T$</p>
<p>so that $r &#x3D; M \cdot r$</p>
<p><strong>Eigenvector Formulation</strong></p>
<p>we could regard $r &#x3D; M \cdot r$ as $1\cdot r &#x3D; M \cdot r$, so that $r$ would be the eigenvector with the eigenvalue 1</p>
<h3 id="2-How-to-solve-PageRank"><a href="#2-How-to-solve-PageRank" class="headerlink" title="2. How to solve PageRank ?"></a>2. How to solve PageRank ?</h3><h4 id="Power-Iteration"><a href="#Power-Iteration" class="headerlink" title="Power Iteration"></a>Power Iteration</h4><p><strong>algorithm</strong> :</p>
<ol>
<li><p>assign each node an initial page rank</p>
</li>
<li><p>repeat until convergence $\sum_i|r_i^{t+1} - r_i^t| &lt;&#x3D; \varepsilon \Leftrightarrow |r^{t+1}-r^t|_1 &lt;&#x3D;varepsilon$:<br>$$<br>\begin{align}<br>&amp;r_j^{t+1} &#x3D; \sum_{i-&gt;j} \frac{r_i^t}{d_i}\<br>\Leftrightarrow &amp;r &#x3D; M\cdot r<br>\end{align}<br>$$</p>
</li>
</ol>
<h4 id="Problem-converge"><a href="#Problem-converge" class="headerlink" title="Problem : converge"></a>Problem : converge</h4><p><strong>problem in converge</strong> :</p>
<p>dead ends problem : no out-links</p>
<p>Spider traps : all out-links within the group</p>
<p><strong>solution</strong> : teleport</p>
<p>At each time step, with the probability $\beta$, follow a link at random; with the probability $1-\beta$, jump to a random node</p>
<p>commonly, $0.8 &lt; \beta &lt;0.9$ </p>
<p>Especially, when surfer arrive at a dead end, it has $1.0$ probability to jump to a random node</p>
<p>Formulation as follow (assume that $\sum_{i} r_i &#x3D; 1$):<br>$$<br>\begin{align}<br>&amp;r_j&#x3D;\sum_{i\rightarrow j}\beta\frac{r_i}{d_i} + \frac1N(1-\beta)\<br>\Leftrightarrow &amp;P &#x3D; \beta \cdot M + \frac1N(1-\beta)<br>\end{align}<br>$$</p>
<h3 id="3-Personalized-PageRank-and-Random-Walks-with-Restarts"><a href="#3-Personalized-PageRank-and-Random-Walks-with-Restarts" class="headerlink" title="3. Personalized PageRank and Random Walks with Restarts"></a>3. Personalized PageRank and Random Walks with Restarts</h3><h4 id="PageRank-PPR-and-RWR"><a href="#PageRank-PPR-and-RWR" class="headerlink" title="PageRank, PPR and RWR"></a>PageRank, PPR and RWR</h4><p><strong>difference</strong> : the points could be teleported to </p>
<p>PageRank : teleport to anywhere on the graph</p>
<p>PPR : teleport to only a subset of graph</p>
<p>RWR : always teleport to the staring point</p>
<h4 id="Random-Walks-with-Restarts"><a href="#Random-Walks-with-Restarts" class="headerlink" title="Random Walks with Restarts"></a>Random Walks with Restarts</h4><p><strong>Idea</strong> : </p>
<ol>
<li>Every node has some importance</li>
<li>Importance gets evenly split among all edges and pushed to the neighbors</li>
</ol>
<p><strong>algorithm</strong> :</p>
<ol>
<li>Given a set of query_nodes $Q$, randomly select a node $q_i$ and make a random walk</li>
<li>make a step to a random neighbor and record visit  or with $1 -\beta$ probability to restart from $q_j \in Q$ </li>
<li>the nodes with the highest visit count have highest proximity to $Q$</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">item = Q.sample_by_weight()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_STEPS):</span><br><span class="line">	item = item.get_random_neighbor()</span><br><span class="line">    item.visited_time += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> random &gt;= beta :</span><br><span class="line">        item = Q.sample_by_weight()</span><br></pre></td></tr></table></figure>

<p><strong>considers</strong> : </p>
<ul>
<li>multiple connections</li>
<li>multiple paths</li>
<li>directed and undirected connection</li>
<li>degree of node</li>
</ul>
<h3 id="4-Matrix-Factorization"><a href="#4-Matrix-Factorization" class="headerlink" title="4. Matrix Factorization"></a>4. Matrix Factorization</h3><h4 id="Embedding-and-Matrix-Factorization"><a href="#Embedding-and-Matrix-Factorization" class="headerlink" title="Embedding and Matrix Factorization"></a>Embedding and Matrix Factorization</h4><p>Given : <strong>measure the similarity of nodes with adjacency matrix $A$</strong></p>
<p>then : $Z^TZ \approx A$</p>
<p>so that : $\arg \min_Z ||A-Z^TZ||_2$</p>
<p>Exactly : we make matrix factorization of adjacency matrix $A$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/06/GNN-Notes-LEC-04/" data-id="cm51u529o0005bww549mz6vm0" data-title="GNN Notes : LEC 04" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-03" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/06/GNN-Notes-LEC-03/" class="article-date">
  <time class="dt-published" datetime="2024-12-06T11:49:08.000Z" itemprop="datePublished">2024-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/06/GNN-Notes-LEC-03/">GNN Notes : LEC 03</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Lec-03-Node-Embedding"><a href="#Lec-03-Node-Embedding" class="headerlink" title="Lec 03 Node Embedding"></a>Lec 03 Node Embedding</h2><h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p><strong>Goal</strong> : Efficient task-independent feature learning for ML with graph</p>
<p><strong>Why Embedding</strong> ?</p>
<ol>
<li>similarity of embedding space indicate that in origin space</li>
<li>encode information</li>
<li>could used in many downstream tasks</li>
</ol>
<h3 id="2-Encoder-and-Decoder"><a href="#2-Encoder-and-Decoder" class="headerlink" title="2. Encoder and Decoder"></a>2. Encoder and Decoder</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">subgraph Embedding Space</span><br><span class="line">a((ZA))</span><br><span class="line">b((ZB))</span><br><span class="line">c((ZC))</span><br><span class="line">d((ZD))</span><br><span class="line">u((ZU))</span><br><span class="line">v((ZV))</span><br><span class="line">end</span><br><span class="line">subgraph Origin Network</span><br><span class="line">1((A))</span><br><span class="line">2((B))</span><br><span class="line">3((C))</span><br><span class="line">4((D))</span><br><span class="line">5((U))</span><br><span class="line">6((V))</span><br><span class="line">1---2</span><br><span class="line">1---3</span><br><span class="line">1---4</span><br><span class="line">5---1</span><br><span class="line">6---1</span><br><span class="line">end</span><br><span class="line">ARROW[Embedding To]</span><br></pre></td></tr></table></figure>

<h4 id="Learning-Node-Embedding"><a href="#Learning-Node-Embedding" class="headerlink" title="Learning Node Embedding"></a>Learning Node Embedding</h4><ol>
<li>Encoder maps from nodes to embeddings</li>
<li>define a similarity function in origin space</li>
<li>Decoder maps from embeddings to similarity scores</li>
<li>optimize the parameters of encoder</li>
</ol>
<h3 id="3-Shallow-Encoding-Embedding-lookup"><a href="#3-Shallow-Encoding-Embedding-lookup" class="headerlink" title="3. Shallow Encoding : Embedding lookup"></a>3. Shallow Encoding : Embedding lookup</h3><p>$$<br>Enc(v_i) &#x3D; z_{v_i} &#x3D; Z \cdot v_i\<br>(v_i)_j &#x3D; \left{ \begin{array}{rcl}<br>0,i\ne j\<br>1,i&#x3D;j<br>\end{array} \right.\<br>Z \in R^{d\times |V|}<br>$$</p>
<p><strong>Weakness</strong> : need to optimize a large number of parameters</p>
<h3 id="4-Random-Walk-Overview"><a href="#4-Random-Walk-Overview" class="headerlink" title="4. Random Walk : Overview"></a>4. Random Walk : Overview</h3><h4 id="Algorithm-of-Random-Walk"><a href="#Algorithm-of-Random-Walk" class="headerlink" title="Algorithm of Random Walk"></a>Algorithm of Random Walk</h4><ol>
<li><p>Run short fixed-length random walks starting from each node $u$ with strategy $R$</p>
</li>
<li><p>collect $N_R(u)$ for each node $u$, where $N_R(u)$ is a multi-set including all node access in step 1</p>
</li>
<li><p>optimize the loss function gradient descent<br>$$<br>\arg \min_{z_u,u\in V} \sum_{u \in V} \sum_{v \in N_R(u)} - \log \frac{\exp(z_u^Tz_v)}{\sum_{n\in V} z_u^Tz_n}<br>$$</p>
</li>
</ol>
<h4 id="Weakness-time-expensive"><a href="#Weakness-time-expensive" class="headerlink" title="Weakness : time expensive"></a>Weakness : time expensive</h4><p>The time complexity of loss function calculating is $O(|N|^2)$, that means it is <strong>time expensive</strong></p>
<h4 id="Solution-Negative-Sample"><a href="#Solution-Negative-Sample" class="headerlink" title="Solution : Negative Sample"></a>Solution : Negative Sample</h4><p>To simplify the loss function’s calculating, use the formula as follow<br>$$<br>\log \frac{\exp(z_u^Tz_v)}{\sum_{n\in V} z_u^Tz_n} \approx \log (\sigma(z_u^Tz_v)) - \sum_{i&#x3D;1}^k \log(\sigma(z_u^Tz_{n_i}))\<br>\sigma(x) &#x3D; \frac{1}{1+\exp(-x)}\<br>$$<br> In the formula, $n_i$ express the node got by sample from random distribution</p>
<p>Proved by the practice, the negative sample have good performance when $k$ just in $[5,20]$</p>
<h3 id="5-The-Random-Walk-Strategy"><a href="#5-The-Random-Walk-Strategy" class="headerlink" title="5. The Random Walk Strategy"></a>5. The Random Walk Strategy</h3><h4 id="5-1-Deep-Walk"><a href="#5-1-Deep-Walk" class="headerlink" title="5.1 Deep Walk"></a>5.1 Deep Walk</h4><p>Just run fixed-length, unbiased random walks</p>
<p>Weakness : too constrained</p>
<h4 id="5-2-Node2Vec"><a href="#5-2-Node2Vec" class="headerlink" title="5.2 Node2Vec"></a>5.2 Node2Vec</h4><p><strong>Idea</strong> : use flexible, biased random walks that can trade off between local and global, where local nodes’ explored by BFS while global by DFS</p>
<p><strong>Super Parameters</strong></p>
<ol>
<li>return parameter $p$ : depend the probability to return to previous node</li>
<li>In-out parameter $q$ : depend the probability to BFS or DFS</li>
</ol>
<p><strong>Example For node2vec</strong></p>
<p>For the graph below, assume previous node is $A$ and current node is $B$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">1((A))</span><br><span class="line">subgraph Distance1</span><br><span class="line">2((B))</span><br><span class="line">3((C))</span><br><span class="line">end</span><br><span class="line">subgraph Distance2</span><br><span class="line">5((E))</span><br><span class="line">6((F))</span><br><span class="line">end</span><br><span class="line">1---2</span><br><span class="line">1---3</span><br><span class="line">2---5</span><br><span class="line">2---6</span><br></pre></td></tr></table></figure>

<p>The next node would depend by random sample from the distribution as follow :<br>$$<br>nextnode &#x3D; \left{\begin{array}{rcl}<br>A,prob &#x3D; 1 &#x2F; p\<br>C,prob &#x3D; 1 &#x2F; q\<br>E,prob &#x3D; 1\<br>F,prob &#x3D; 1<br>\end{array}\right.\<br>$$<br>Attention : the probabilities is <strong>not normalized</strong></p>
<h3 id="6-Entire-Graph-Embedding"><a href="#6-Entire-Graph-Embedding" class="headerlink" title="6. Entire Graph Embedding"></a>6. Entire Graph Embedding</h3><h4 id="6-1-Sum-or-average"><a href="#6-1-Sum-or-average" class="headerlink" title="6.1 Sum or average"></a>6.1 Sum or average</h4><p><strong>algorithm</strong> :</p>
<ol>
<li>run a standard node embedding technique on graph G</li>
<li>sum or average all node embeddings in the G</li>
</ol>
<h4 id="6-2-Virtual-Node"><a href="#6-2-Virtual-Node" class="headerlink" title="6.2 Virtual Node"></a>6.2 Virtual Node</h4><p><strong>algorithm</strong> : </p>
<ol>
<li>use a virtual node represent the entire graph, maybe have a series of edges with node in the graph</li>
<li>run a standard node embedding technique to get the virtual node’s embedding</li>
</ol>
<h4 id="6-3-Anonymous-Walk-Embedding-v1"><a href="#6-3-Anonymous-Walk-Embedding-v1" class="headerlink" title="6.3 Anonymous Walk Embedding : v1"></a>6.3 Anonymous Walk Embedding : v1</h4><p><strong>algorithm</strong> : </p>
<ol>
<li>run a random walk on the graph $G$</li>
<li>anonymization : for each node in a random walk result, replace its name with the index of the first time it occur</li>
<li>for all of anonymous walk produced by 2, count the number of all kinds of anonymous walk</li>
<li>Embedding $Z_G[i]$ represent the score or probability of $i$-th anonymous walk</li>
</ol>
<p><strong>the dimension of embedding $Z_G$</strong>:</p>
<p>Because the kind number of anonymous walk <strong>depend on the length of walk</strong>, so the dimension of embedding depend on it too.</p>
<p><strong>How many random walks m do we need?</strong></p>
<p>We want the distribution to have <strong>error of more than $\varepsilon$ with probability less than $\delta$</strong><br>$$<br>m &#x3D; \lceil \frac 2 {\varepsilon^2(\log(2^n-2)-\log(\delta))} \rceil<br>$$</p>
<h4 id="6-4-Anonymous-Walk-Embedding-v2"><a href="#6-4-Anonymous-Walk-Embedding-v2" class="headerlink" title="6.4 Anonymous Walk Embedding : v2"></a>6.4 Anonymous Walk Embedding : v2</h4><p><strong>signals</strong> : </p>
<p>$\eta$ : the number of samples, the “sample” mean the sum of window number could be extracted from all anonymous walk</p>
<p>$Z$ : the embeddings of walks, $z_i$ express $i$-th specific type of anonymous walk</p>
<p>$\Delta$ : the window size</p>
<p>$z_G$ : the embedding of the entire graph $G$</p>
<p>$w_{i,j}$ : the $j$-th anonymous walk in the series sampled starting from node $i$  </p>
<p><strong>algorithm</strong> :</p>
<p>learn the embeddings of anonymous walks $Z &#x3D; {z_i|i&#x3D;1,2,…,\eta}$ as well as $z_G$</p>
<p>a) for each node i, starting from it and sample anonymous walks randomly, get a series of walks called $[w_{i1},w_{i2},…,w_{iT}]$</p>
<p>b) for a window size $\Delta$, learn to predict walks that co-occur<br>$$<br>\arg \min_{Z,z_G,b,U} \sum_{i \in G }\sum_{t&#x3D;\Delta}^{T-\Delta} -\log P(w_{i,t}|{w_{i,t-\Delta},w_{i,t-\Delta+1},…,w_{i,t+\Delta},z_G})\<br>P(w_{i,t}|{w_{i,t-\Delta},w_{i,t-\Delta+1},…,w_{i,t+\Delta},z_G}) &#x3D; \frac{\exp(y(w_{i,t}))}{\sum_{j&#x3D;1}^\eta \exp(y(w_j))}\<br>y(w) &#x3D; b + U (cat(\frac 1 {2\Delta}\sum_{i&#x3D;-\Delta}^\Delta z_i,z_G))<br>$$</p>
<h3 id="7-How-to-use-Embeddings"><a href="#7-How-to-use-Embeddings" class="headerlink" title="7. How to use Embeddings"></a>7. How to use Embeddings</h3><ul>
<li><p><strong>Clustering</strong> by point $z_i$</p>
<ul>
<li>community detection</li>
</ul>
</li>
<li><p><strong>Node classification</strong> by point $z_i$</p>
<ul>
<li>predict the label of node</li>
</ul>
</li>
<li><p><strong>Link prediction</strong> : predict edge $(i,j)$ based on $(z_i,z_j)$</p>
<ul>
<li>concatenate : $[z_i,z_j]$</li>
<li>Hadamard :  $z_i * z_j$</li>
<li>Sum &#x2F; Avg : $z_i + z_j$</li>
<li>Distance : $||z_i-z_j||_2$</li>
</ul>
</li>
<li><p><strong>Graph classification</strong> by $z_G$</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/06/GNN-Notes-LEC-03/" data-id="cm51u529m0004bww591zs6mfa" data-title="GNN Notes : LEC 03" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GNN-Notes-LEC-02" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/06/GNN-Notes-LEC-02/" class="article-date">
  <time class="dt-published" datetime="2024-12-06T11:49:05.000Z" itemprop="datePublished">2024-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/06/GNN-Notes-LEC-02/">GNN Notes : LEC 02</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Lec-02-Traditional-feature-base-method-for-GNN"><a href="#Lec-02-Traditional-feature-base-method-for-GNN" class="headerlink" title="Lec 02 Traditional feature-base method for GNN"></a>Lec 02 Traditional feature-base method for GNN</h2><h3 id="1-Node-Level-Feature"><a href="#1-Node-Level-Feature" class="headerlink" title="1. Node-Level Feature"></a>1. Node-Level Feature</h3><h4 id="Node-Degree"><a href="#Node-Degree" class="headerlink" title="Node Degree"></a>Node Degree</h4><p><strong>category</strong> : importance-based, structure-based</p>
<p>weakness : treat all neighboring nodes equally</p>
<h4 id="Node-Centrality"><a href="#Node-Centrality" class="headerlink" title="Node Centrality"></a>Node Centrality</h4><p><strong>category</strong> : importance-based</p>
<p><strong>core</strong> : take the node importance in a graph</p>
<h5 id="eigenvector-centrality"><a href="#eigenvector-centrality" class="headerlink" title="eigenvector centrality"></a>eigenvector centrality</h5><p><strong>core</strong> : v is important if its neighboring nodes are important</p>
<p><strong>formula</strong> :<br>$$<br>\begin{align}<br>&amp;c_v &#x3D; \frac{1}{\lambda} \sum_{u\in N(v)} c_u\</p>
<p>\Leftrightarrow&amp;\lambda c &#x3D; A c<br>\end{align}<br>$$</p>
<h5 id="bewteeness-centrality"><a href="#bewteeness-centrality" class="headerlink" title="bewteeness centrality"></a>bewteeness centrality</h5><p><strong>core</strong> : v is important if v lies on many shortest paths between two nodes</p>
<p><strong>formula</strong> :</p>
<p>use $p(u,v)$ express the number of shortest path between $u$ and $v$</p>
<p>use $p(v,s,t)$ express the number of shortest path between $s$ and $t$ via $v$<br>$$<br>c_v &#x3D; \sum_{v\ne s\ne t}\frac{p(s,t)}{p(s,t,v)}<br>$$</p>
<h5 id="closeness-centrality"><a href="#closeness-centrality" class="headerlink" title="closeness centrality"></a>closeness centrality</h5><p><strong>core</strong> : v is important if v has small shortest path lengths to all other nodes</p>
<p><strong>formula</strong> : </p>
<p>use $p(u,v)$ express the shortest path length between node $u$ and $v$<br>$$<br>c_v &#x3D; \frac 1 {\sum_{u\ne v}p(u,v)}<br>$$</p>
<h4 id="Clustering-coefficient"><a href="#Clustering-coefficient" class="headerlink" title="Clustering coefficient"></a>Clustering coefficient</h4><p><strong>category</strong> : structure-based</p>
<p><strong>formula</strong> : </p>
<p>use $e(N(v))$ express the number of edge among $N(v)$<br>$$<br>c_v &#x3D; \frac{e(N(v))}{C_{k_v}^2}<br>$$</p>
<h4 id="Graphlet-Degree-Vector"><a href="#Graphlet-Degree-Vector" class="headerlink" title="Graphlet Degree Vector"></a>Graphlet Degree Vector</h4><p><strong>category</strong> : structure-based</p>
<p><strong>core</strong> : use the number of graphlet with node $v$ express $v$’s feature</p>
<h3 id="2-Link-Level-Feature"><a href="#2-Link-Level-Feature" class="headerlink" title="2. Link-Level Feature"></a>2. Link-Level Feature</h3><h4 id="Task-of-Link-Prediction"><a href="#Task-of-Link-Prediction" class="headerlink" title="Task of Link Prediction"></a>Task of Link Prediction</h4><p>Given : existing links</p>
<p>Aim : predict new links</p>
<p>Key : design features to express a pair of nodes</p>
<h4 id="Formulations-of-link-predictions"><a href="#Formulations-of-link-predictions" class="headerlink" title="Formulations of link predictions"></a>Formulations of link predictions</h4><h5 id="links-missing-at-ramdon"><a href="#links-missing-at-ramdon" class="headerlink" title="links missing at ramdon"></a>links missing at ramdon</h5><p><strong>core</strong> : self-supervised</p>
<p><strong>methodology</strong> :</p>
<ol>
<li>remove links randomly</li>
<li>predict them</li>
</ol>
<h5 id="links-over-time"><a href="#links-over-time" class="headerlink" title="links over time"></a>links over time</h5><p><strong>core</strong> : self-regression</p>
<p>Given : $G[t_0,t_0’]$  express graph $G$ at time $t_0’$</p>
<p>Output : $L$ express a list of links that be predicted to appear at time $t_1’$</p>
<p>Evaluation : take top $n$ links in $L$ and then count how many links in it appear at time $t_1’$</p>
<p>Methodology : </p>
<ol>
<li>for each $(x,y)$, compute its score $c(x,y)$</li>
<li>decreasing score $c(x,y)$</li>
<li>select top $n$ as predicted links</li>
</ol>
<h4 id="Distance-based-Feature"><a href="#Distance-based-Feature" class="headerlink" title="Distance-based Feature"></a>Distance-based Feature</h4><p>mainly include : shortest path distance  between two nodes</p>
<p>use $s(u,v)$ express the length of shortest path between node $u$ and node $v$, and use $s(u,v)$ as the feature of $(u,v)$</p>
<p>weakness : ignore the neighborhood’s degree</p>
<h4 id="Local-Neighborhood-Overlap"><a href="#Local-Neighborhood-Overlap" class="headerlink" title="Local Neighborhood Overlap"></a>Local Neighborhood Overlap</h4><p><strong>common neighborhood</strong> </p>
<p>$|N(v_1)\cap N(v_2)|$</p>
<p><strong>Jaccard’s coefficient</strong></p>
<p>$\frac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}$</p>
<p><strong>Adamic-Adar index</strong></p>
<p>encoding the <strong>node importance</strong> also</p>
<p>$\sum_{u\in N(v_1)\cap N(v_2)} \frac 1{\log(k_u)}$</p>
<p>Limitation : metric will be 0 if two nodes has no common neighbors but they may connected in the furture</p>
<h4 id="Global-Neighborhood-Overlap"><a href="#Global-Neighborhood-Overlap" class="headerlink" title="Global Neighborhood Overlap"></a>Global Neighborhood Overlap</h4><p>core :  use the number of paths of all lengths between given nodes</p>
<p>calculation : calculate by the power of Adjacency Matrix</p>
<p>Katz index’s formulation :<br>$$<br>s_{u,v} &#x3D; \sum_{l&#x3D;1}^{\infty} \beta^l A^l\<br>S &#x3D; (I-\beta A)^{-1}-I\<br>where\ 0&lt; \beta &lt; 1<br>$$</p>
<h3 id="3-Graph-Level-Feature"><a href="#3-Graph-Level-Feature" class="headerlink" title="3. Graph-Level Feature"></a>3. Graph-Level Feature</h3><h4 id="Kernel-Methods"><a href="#Kernel-Methods" class="headerlink" title="Kernel Methods"></a>Kernel Methods</h4><p><strong>Idea</strong> : design kernel instead of feature vector</p>
<p><strong>Introduction</strong> :</p>
<ol>
<li><p>$K(G,G’)$ measures the similarity between $G$ and $G’$</p>
</li>
<li><p>Kernel metrix $k&#x3D;(K(G,G’))_{G,G’}$ must be positive semidefinite</p>
</li>
<li><p>exists a feature represent $\phi(\cdot)$ such that $K(G,G’) &#x3D; \phi(G) \phi(G’)$</p>
</li>
</ol>
<h4 id="Core-Idea-of-Graph-Kernels"><a href="#Core-Idea-of-Graph-Kernels" class="headerlink" title="Core Idea of Graph Kernels"></a>Core Idea of Graph Kernels</h4><p>Bags-of-words is the main idea use in graph feature</p>
<h4 id="Graphlet-Features"><a href="#Graphlet-Features" class="headerlink" title="Graphlet Features"></a>Graphlet Features</h4><p>Main Idea : Bags of Graphlets</p>
<p>Given : graph $G$, graphlet list $G_k &#x3D; (g_1,g_2,…,g_{n_k})$, graphlet count vector $f_G \in R^{n_k}$, where $(f_G)_i &#x3D; #(g_i \in G)$</p>
<p>Output : $K(G,G’) &#x3D; f_G^T f_{G^{‘}}$</p>
<p>Problem : if $G$ and $G’$ has different sizes, that woudl greatly skew value</p>
<p>Solutions : normalization by $h_G &#x3D; \frac{f_G}{sum(f_G)}$ and $K(G,G’) &#x3D; h_G^T h_{G^{‘}}$</p>
<p>Limitations : NP-hard problem, time expensive</p>
<h4 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h4><p><strong>Idea</strong> ：use neighborhood structure to iteratively enrich node vocabulary, or bags of colors</p>
<p><strong>Methodology</strong> :</p>
<ol>
<li><p>Given graph G with node set V</p>
</li>
<li><p>Init : $c^0(v)$</p>
</li>
<li><p>for K steps :</p>
<p>​	$c^{(k+1)}(v) &#x3D; hash({c^{(k)}(v),{c^{(k)}(u)}_{u\in N(v)}})$</p>
</li>
<li><p>count numbers of nodes with given color</p>
</li>
</ol>
<p>Time Complexity : $K |E|$ </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bitlfy.github.io/2024/12/06/GNN-Notes-LEC-02/" data-id="cm51u529l0003bww53lb32jyi" data-title="GNN Notes : LEC 02" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/12/24/GNN-Notes-LEC-10/">GNN-Notes-LEC-10</a>
          </li>
        
          <li>
            <a href="/2024/12/24/GNN-Notes-LEC-09/">GNN-Notes-LEC-09</a>
          </li>
        
          <li>
            <a href="/2024/12/24/GNN-Notes-LEC-07-08/">GNN-Notes-LEC-07-08</a>
          </li>
        
          <li>
            <a href="/2024/12/24/GNN-Notes-LEC-06/">GNN-Notes-LEC-06</a>
          </li>
        
          <li>
            <a href="/2024/12/15/GNN-Notes-LEC-05/">GNN-Notes-LEC-05</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Lin Fangyv<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>